{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa58775",
   "metadata": {},
   "source": [
    "# Q2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1079dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bandit(arm):   #function to simulate the two-armed slot machine, taking the arm we want to play as input\n",
    "                   #and returns a sample from a Bernoulli distrubution for each arm \n",
    "    mu1 = 0.6\n",
    "    mu2 = 0.4\n",
    "    \n",
    "    if arm == 1:   \n",
    "        return np.random.binomial(1, mu1)\n",
    "    elif arm == 2:\n",
    "        return np.random.binomial(1, mu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad0ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decreasing(n, epsilon_sequence):     \n",
    "\n",
    "    def choose_arm(epsilon, wins_1, plays_1, wins_2, plays_2):   #function that returns which arm we should play at each step\n",
    "                                                                 #of the epsilon decreasing strategy, given a probability   \n",
    "                                                                 #epsilon and the number of plays and wins for each arm\n",
    "        \n",
    "        if np.random.binomial(1, epsilon) == 1:   #returns a uniformly random arm with probability epsilon               \n",
    "            arm = np.random.binomial(1, 0.5) + 1\n",
    "            return arm\n",
    "        \n",
    "        else:                           #else, plays the arm with the best win rate, or a random arm if the win rates are equal\n",
    "            win_rate_1 = wins_1/plays_1\n",
    "            win_rate_2 = wins_2/plays_2\n",
    "            if win_rate_1 > win_rate_2:\n",
    "                return 1\n",
    "            elif win_rate_2 > win_rate_1:\n",
    "                return 2\n",
    "            else:\n",
    "                return np.random.binomial(1,0.5) + 1\n",
    "        \n",
    "    arms = np.zeros(n, int)            \n",
    "    rewards = np.zeros(n, int)         \n",
    "    \n",
    "    arms[0] = 1                        #play the first arm once\n",
    "    rewards[0] = bandit(1)\n",
    "\n",
    "    arms[1] = 2                        #play the second arm once\n",
    "    rewards[1] = bandit(2)\n",
    "\n",
    "    plays_1 = 1; plays_2 = 1               #these variables are passed to choose_arm for efficiency\n",
    "    wins_1 = rewards[0]; wins_2 = rewards[1]\n",
    "\n",
    "    for i in range(2, n):              #then, follow the epsilon decreasing strategy until n steps have occured\n",
    "        \n",
    "        arm = choose_arm(epsilon_sequence[i], wins_1, plays_1, wins_2, plays_2)    \n",
    "        \n",
    "        if arm == 1:                  \n",
    "            arms[i] = 1\n",
    "            plays_1 += 1\n",
    "            r = bandit(1)\n",
    "            rewards[i] = r            #record the arm played and reward gained at each step\n",
    "            wins_1 += r\n",
    "        else:\n",
    "            arms[i] = 2\n",
    "            plays_2 += 1\n",
    "            r = bandit(2)\n",
    "            rewards[i] = r\n",
    "            wins_2 += r\n",
    "    \n",
    "    return (arms, rewards)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd030cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thompson_sampling(n):   \n",
    "    \n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "\n",
    "    arms = np.zeros(n, int)           \n",
    "    rewards = np.zeros(n, int)\n",
    "\n",
    "    s1 = 0; f1 = 0; s2 = 0; f2 = 0    #s1 and f1 are successes and failures for arm 1, vice versa for arm 2,  \n",
    "                                      #as in the lecture slides\n",
    "    for i in range(n):\n",
    "        \n",
    "        m1 = np.random.beta(alpha + s1, beta + f1)   #at each step, sample from a beta distribution\n",
    "        m2 = np.random.beta(alpha + s2, beta + f2)\n",
    "        \n",
    "        if m1 >= m2:             #play arm 1 if m1 >= m2\n",
    "            arms[i] = 1\n",
    "            r = bandit(1)        \n",
    "            rewards[i] = r\n",
    "            if r == 1: \n",
    "                s1 += 1\n",
    "            else: f1 += 1\n",
    "                \n",
    "        else:                   #play arm 2 otherwise\n",
    "            arms[i] = 2\n",
    "            r = bandit(2)       \n",
    "            rewards[i] = r\n",
    "            if r == 1: \n",
    "                s2 += 1\n",
    "            else: f2 += 1\n",
    "\n",
    "    return (arms, rewards)      #return the arms played and rewards gained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81bcbb",
   "metadata": {},
   "source": [
    "# Q2b \n",
    "\n",
    "Given $\\epsilon _{n} = \\min(1, C/n) $, the epsilon decreasing algorithm will play completely randomly for $\\left \\lfloor C \\right \\rfloor$ steps because if $n$ is less than $C$, then $C/n >1$ and so $\\epsilon_n = 1$. Then, the arm with the best win rate will be chosen more often at each step, until it is chosen the vast majority of the time. Thus, for small values of C, we do not play each arm many times before deciding which arm is best, so we may observe by chance that the arm with the worse \"true win rate\" actually has a better observed win rate, meaning that we may play the worse arm the majority of the time, leading to low average reward over time. This becomes increasingly unlikely as $C$ increases, but then we are also playing randomly for a long time for large values of $C$, leading to a large lost reward early on, i.e a higher regret. So its a trade-off between making sure the arm we choose is actually best and getting the best rewards early on.\n",
    "\n",
    "We can observe this in our implementation if we run the epsilon decreasing algorithm 20 times with a small $C$ and 20 times with a high $C$ and calculate the average reward for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8828b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587\n",
      "0.5956\n",
      "0.6007\n",
      "0.5988\n",
      "0.6035\n",
      "0.594\n",
      "0.6049\n",
      "0.3966\n",
      "0.5258\n",
      "0.5991\n",
      "0.5994\n",
      "0.6015\n",
      "0.6066\n",
      "0.3955\n",
      "0.5996\n",
      "0.6007\n",
      "0.5948\n",
      "0.3957\n",
      "0.4022\n",
      "0.6054\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "C = 1            #low value of C\n",
    "seq = []\n",
    "for i in range(1,n+1):\n",
    "    seq.append(min(1, C/(i)))\n",
    "    \n",
    "for i in range(20):\n",
    "    print(sum(epsilon_decreasing(n, seq)[1])/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21412d9b",
   "metadata": {},
   "source": [
    "We want our average reward to be as close as possible to $0.6$ because in this case, $\\max(\\mu _1, \\mu_2) = 0.6$. In most of the above runs, our average reward is close to 0.6, but in a few, it is close to 0.4 which means that the worse arm was played the majority of the time because it had (by chance) a better empirical win rate early on. In other words, when $C$ is small, the algorithm can sometimes \"choose\" the wrong arm and stick with it.\n",
    "\n",
    "Now lets see the results for a high value of $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ac0666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5994\n",
      "0.5898\n",
      "0.5974\n",
      "0.5857\n",
      "0.5942\n",
      "0.5896\n",
      "0.6029\n",
      "0.5942\n",
      "0.5925\n",
      "0.5951\n",
      "0.6022\n",
      "0.5911\n",
      "0.5926\n",
      "0.5909\n",
      "0.5904\n",
      "0.5951\n",
      "0.5994\n",
      "0.5883\n",
      "0.5957\n",
      "0.5925\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "C = 100           #high value of C\n",
    "seq = []\n",
    "for i in range(1,n+1):\n",
    "    seq.append(min(1, C/(i)))\n",
    "    \n",
    "for i in range(20):\n",
    "    print(sum(epsilon_decreasing(n, seq)[1])/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c04f79",
   "metadata": {},
   "source": [
    "We see that the true best arm was played most of the time here in every run, because we had many early steps to collect information about each arm, but the average reward is slightly lower than the runs which correctly identified the best arm in the runs with small $C$. This shows us that the regret is higher with a large value of $C$ because of the longer time spent playing randomly early on.\n",
    "\n",
    "# Q2c\n",
    "\n",
    "$\\frac{C}{n^2}$ decreases much faster than $\\frac{C}{n}$, so we would expect to see a similar pattern to above, but more extreme, with the algorithm \"settling\" on one arm sooner for small values of $C$. Effectively, this is closer to the \"$\\epsilon$-first\" strategy. Let's repeat the above tests with the same values of $C$ to see this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e680cc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599\n",
      "0.5931\n",
      "0.4008\n",
      "0.5962\n",
      "0.5978\n",
      "0.6003\n",
      "0.4027\n",
      "0.6002\n",
      "0.601\n",
      "0.4\n",
      "0.3987\n",
      "0.602\n",
      "0.5962\n",
      "0.6054\n",
      "0.5983\n",
      "0.6113\n",
      "0.5999\n",
      "0.3911\n",
      "0.6003\n",
      "0.4002\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "C = 1            #low value of C\n",
    "seq = []\n",
    "for i in range(1,n+1):\n",
    "    seq.append(min(1, C/(i**2)))\n",
    "    \n",
    "for i in range(20):\n",
    "    print(sum(epsilon_decreasing(n, seq)[1])/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35cc58",
   "metadata": {},
   "source": [
    "Here, it has chosen the wrong arm 6 times out of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abf3a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5939\n",
      "0.6006\n",
      "0.596\n",
      "0.6023\n",
      "0.5978\n",
      "0.5968\n",
      "0.5855\n",
      "0.604\n",
      "0.6061\n",
      "0.6022\n",
      "0.5941\n",
      "0.6034\n",
      "0.6061\n",
      "0.6024\n",
      "0.5911\n",
      "0.6007\n",
      "0.5943\n",
      "0.6026\n",
      "0.5899\n",
      "0.3978\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "C = 100           #high value of C\n",
    "seq = []\n",
    "for i in range(1,n+1):\n",
    "    seq.append(min(1, C/(i**2)))\n",
    "    \n",
    "for i in range(20):\n",
    "    print(sum(epsilon_decreasing(n, seq)[1])/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274620d",
   "metadata": {},
   "source": [
    "Even for a large value of $C$, the algorithm chose the wrong arm once, although when choosing the correct arm, the average rewards appear higher than the first $\\epsilon$ sequence because once choosing the right arm, the chances of playing randomly become extremely small very quickly.\n",
    "\n",
    "# Q2d\n",
    "\n",
    "Lets run our Thompson sampling algorithm and compute average rewards for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6cd6003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6012\n",
      "0.593\n",
      "0.6027\n",
      "0.6044\n",
      "0.6053\n",
      "0.6004\n",
      "0.5975\n",
      "0.5969\n",
      "0.6025\n",
      "0.6028\n",
      "0.5998\n",
      "0.6114\n",
      "0.5991\n",
      "0.6018\n",
      "0.5984\n",
      "0.5942\n",
      "0.6051\n",
      "0.6031\n",
      "0.6054\n",
      "0.6024\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "for i in range(20):\n",
    "    print(sum(Thompson_sampling(n)[1])/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141495c3",
   "metadata": {},
   "source": [
    "This strategy appears to work very well because all the average rewards are extremely close to $0.6$. Thompson sampling does not exhibit the same \"choosing the wrong arm\" phenomenon as the $\\epsilon$-decreasing strategy. The average rewards also seem to be higher or around the same as $\\epsilon$-decreasing, when ignoring the runs where the wrong arm was chosen. To see which algorithm is better, lets run the Thompson algorithm , the first $\\epsilon$-decreasing with large $C$, and the second $\\epsilon$-decreasing with large $C$ each 100 times and compute an \"average of averages\" for the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba1cff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5983670000000003\n",
      "0.5652259999999999\n",
      "0.5563050000000002\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "C == 100\n",
    "total = 0                              #Thompson sampling\n",
    "for i in range(100):\n",
    "    total += sum(Thompson_sampling(n)[1])/n\n",
    "\n",
    "print(total/100)\n",
    "\n",
    "\n",
    "\n",
    "seq = []\n",
    "for i in range(1,n+1):\n",
    "    seq.append(min(1, C/(i)))          #epsilon decreasing with C/n\n",
    "total = 0\n",
    "for i in range(100):\n",
    "    total += sum(epsilon_decreasing(n, seq)[1])/n\n",
    "\n",
    "print(total/100)\n",
    "\n",
    "\n",
    "seq = []\n",
    "for i in range(1,n+1):\n",
    "    seq.append(min(1, C/(i**2)))         #epsilon decreasing with C/n^2\n",
    "total = 0\n",
    "for i in range(100):\n",
    "    total += sum(epsilon_decreasing(n, seq)[1])/n\n",
    "\n",
    "print(total/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58308191",
   "metadata": {},
   "source": [
    "This shows us that Thompson sampling is best because it has the highest average reward over many runs, which is consistent with the theory. This also demonstrates that sequences that decrease slower are better for the $\\epsilon$-decreasing algorithm, because the average reward over many runs is higher for the first sequence than the second."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
